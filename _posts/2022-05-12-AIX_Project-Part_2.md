---

layout : post
title : Part 2. MCTS 구현

---

**이번 포스트에서는 MCTS를 구현하는 과정을 소개합니다**

---

## **MCTS의 작동 원리**

 * MCTS *Monte Carlo Tree Search*의 탐색과정

![an image alt text]({{ site.baseurl }}/images/AIX_Project_Part02/mcts01.png "MCTS progress scheme")
<br />
<br />
 MCTS의 탐색과정은 다음의 4가지 단계로 이루어집니다.
<br />
<br />
1. *Selection*
<br />

 기존에 가지고 있는 탐색트리 내에서 트리 정책을 통해서 리프노드를 선택합니다.
 트리 정책은 알고리즘마다 세세한 부분은 차이가 있을 수 있지만, 대개의 경우, 트리정책은 해당 노드의
 플레이어가 자신의 이득을 최대로 할 수 있는 노드를 선택하는 것을 의미하도록 설정됩니다.


2. *Expansion*
<br />

 *Selection* 단계에서 도달한 리프노드에서 자식노드를 추가합니다. 추가된 노드의 추정 가치나
 방문횟수같은 통계 값들은 모두 0으로 설정됩니다.


3. *Simulation*
<br />

 *Expansion* 단계에서 추가된 자식노드의 추정 가치를 결정하는 단계입니다. 자식노드에서
 시작하여, 완전 무작위행동을 하는 정책, 또는 roll out policy 라는 특정 정책을 사용하여
 그 노드에서 게임을 끝까지 진행하였을 때 어떤 결과가 나오는지 확인합니다. 시뮬레이션을 한
 결과를 그 노드의 가치 추정값으로 사용합니다.  
 시뮬레이션을 하는 횟수가 증가할 수록 노드의 가치 추정값은 정확해지며, roll out policy가
 우수한 성능을 가질 수록 그 노드의 가치 추정값이 정확해집니다.


4. *Backpropagation*
<br />

 추가된 노드의 가치 추정값, 또는 방문횟수와 같은 모든 통계적 데이터들을 그 노드에 도달하기
 위해서 거쳤던 모든 노드에 합산합니다.

<br />
<br />

 (1) ~ (4)의 모든 과정을 여러번 반복하면서 트리는 점점 확장되고, 트리가 가지고 있는
 노드의 가치 추정값들은 점차 정확해집니다. 따라서 MCTS를 통해서 행동하는 AI는 단순히 rollout
 policy를 사용하는 AI에 비해서 더 성능이 우수합니다.

<br />
<br />

---

## **Alphago Zero의 MCTS**
<br />
<br />

![an image alt text]({{ site.baseurl }}/images/AIX_Project_Part02/mcts01.png "MCTS progress scheme")
<br />
<br />
알파고 제로는 인간의 지식을 사용하지 않고, 오로지 MCTS와 자가대국, 강화학습만으로 인간의
실력을 아득히 뛰어넘은 바둑 AI입니다.  
알파고 제로의 등장 이전에도 바둑을 MCTS를 사용해서 해결하려는 AI가 없었던 것은 아니었지만
바둑의 특성상 너무나 많은 경우의 수를 효율적으로 탐색하기에는 무리가 있었습니다.  
알파고 제로는 기존의 MCTS를 **탐색 Depth**와 **탐색 Width** 2가지 측면으로 개선하여
바둑을 해결할 수 있었습니다.
<br />
<br />

* 탐색 Depth 개선
<br />

기존의 MCTS는 새로 추가되는 노드의 가치를 추정하기 위해서 그 노드를 시작으로 *Simulation*
을 여러번하여 게임을 끝까지 할 때 나오는 결과를 평균내어야 했습니다. 하지만 바둑은
게임의 결과를 얻기까지 (게임의 초반이라면) 약 300수 정도를 더 두어야 하고, 이렇게 결과상태까지의  
 거리가 멀다면 얻게된 결과도 노드의 가치와 연관성이 매우 약하게 됩니다. 때문에 정확한 가치 추정값을
 얻기위해서는 매우 많은 *Simulation*이 필요하고, 이는 짧은 시간내에 문제를 해결할 수 없는
 원인이 됩니다.  
 알파고 제로는 *Simulation*단계를 신경망에 해당 노드의 State를 입력하여 가치를 추정하는
 것으로 대체합니다. 이렇게 함으로써 탐색의 깊이를 게임의 끝까지 하지 않고 효율적으로
 줄일 수 있습니다.

 * 탐색 Width 개선
<br />

한 상태에서 가능한 action의 가짓수(branch factor)가 크다는 것도 MCTS에서 문제가
됩니다. 바둑의 가능한 action의 수를 대략 361이라고 한다면 N 단계까지 탐색을 진행하기
위해서는 361^N 개의 노드가 완성되어야 합니다. 알파고 제로는 이러한 문제를 트리정책을
학습시켜서 해결하였습니다. 한 상태에서 전이가능한 다음 상태들에 대해서, *그럴듯한* 상태에
 높은 확률을 부여해서 *Selection*단계에서 방문할 확률이 높도록 설정하였습니다. 이러한
 효과로, 트리가 모든 상태를 균등하게 탐색하지 않고, 유력한 상태를 주로 탐색하면서 탐색의
 효율을 증가시켰습니다.





